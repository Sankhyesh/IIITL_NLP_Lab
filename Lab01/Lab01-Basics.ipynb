{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {},
   "source": [
    "## Introduction to Lab 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {},
   "source": [
    "This lab is an introduction to some basic test processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-shuttle",
   "metadata": {},
   "source": [
    "So lets first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The voice that navigated was definitely that of a machine, and yet you could tell that the machine was a woman, which hurt my mind a little.\\n How can machines have genders?\\n The machine also had an American accent.\\n How can machines have nationalities?\\n This can't be a good idea, making machines talk like real people, can it?\\n Giving machines humanoid identities?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {},
   "source": [
    "Now lets try to split the text based on spaces (default function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "careful-cleaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'voice',\n",
       " 'that',\n",
       " 'navigated',\n",
       " 'was',\n",
       " 'definitely',\n",
       " 'that',\n",
       " 'of',\n",
       " 'a',\n",
       " 'machine,',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'could',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'was',\n",
       " 'a',\n",
       " 'woman,',\n",
       " 'which',\n",
       " 'hurt',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'a',\n",
       " 'little.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'genders?',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'also',\n",
       " 'had',\n",
       " 'an',\n",
       " 'American',\n",
       " 'accent.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'nationalities?',\n",
       " 'This',\n",
       " \"can't\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea,',\n",
       " 'making',\n",
       " 'machines',\n",
       " 'talk',\n",
       " 'like',\n",
       " 'real',\n",
       " 'people,',\n",
       " 'can',\n",
       " 'it?',\n",
       " 'Giving',\n",
       " 'machines',\n",
       " 'humanoid',\n",
       " 'identities?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {},
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended to the words.\n",
    "So we need to find a way to remove those characters... but, before we do that, lets see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regional-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American',\n",
       " 'Giving',\n",
       " 'How',\n",
       " 'The',\n",
       " 'This',\n",
       " 'a',\n",
       " 'accent.',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'be',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'could',\n",
       " 'definitely',\n",
       " 'genders?',\n",
       " 'good',\n",
       " 'had',\n",
       " 'have',\n",
       " 'humanoid',\n",
       " 'hurt',\n",
       " 'idea,',\n",
       " 'identities?',\n",
       " 'it?',\n",
       " 'like',\n",
       " 'little.',\n",
       " 'machine',\n",
       " 'machine,',\n",
       " 'machines',\n",
       " 'making',\n",
       " 'mind',\n",
       " 'my',\n",
       " 'nationalities?',\n",
       " 'navigated',\n",
       " 'of',\n",
       " 'people,',\n",
       " 'real',\n",
       " 'talk',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'the',\n",
       " 'voice',\n",
       " 'was',\n",
       " 'which',\n",
       " 'woman,',\n",
       " 'yet',\n",
       " 'you']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(sentence.split()) # splitting based on spaces\n",
    "vocab = sorted(set(tokens)) # sorting and removing duplicates by using set()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {},
   "source": [
    "We can see that the order has the numbers first, followerd by capital and then lower case letters (all alphabetically sorted). We also see that some repeating words appear only once in the vocabulary list. Let's compate the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "periodic-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 62\n",
      "vocab: 47\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(\"tokens:\", tokens_len)\n",
    "print(\"vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {},
   "source": [
    "Lets try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ultimate-litigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {},
   "source": [
    "Is not easy to see, but some columns contain multiple rows showing 1, whereas the rest is all one 1 per column. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "realistic-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>American</th>\n",
       "      <th>Giving</th>\n",
       "      <th>How</th>\n",
       "      <th>The</th>\n",
       "      <th>This</th>\n",
       "      <th>a</th>\n",
       "      <th>accent.</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>...</th>\n",
       "      <th>talk</th>\n",
       "      <th>tell</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>voice</th>\n",
       "      <th>was</th>\n",
       "      <th>which</th>\n",
       "      <th>woman,</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Giving</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          American  Giving  How  The  This  a  accent.  also  an  and  ...  \\\n",
       "American         1       0    0    0     0  0        0     0   0    0  ...   \n",
       "Giving           0       1    0    0     0  0        0     0   0    0  ...   \n",
       "How              0       0    1    0     0  0        0     0   0    0  ...   \n",
       "How              0       0    1    0     0  0        0     0   0    0  ...   \n",
       "The              0       0    0    1     0  0        0     0   0    0  ...   \n",
       "...            ...     ...  ...  ...   ... ..      ...   ...  ..  ...  ...   \n",
       "was              0       0    0    0     0  0        0     0   0    0  ...   \n",
       "which            0       0    0    0     0  0        0     0   0    0  ...   \n",
       "woman,           0       0    0    0     0  0        0     0   0    0  ...   \n",
       "yet              0       0    0    0     0  0        0     0   0    0  ...   \n",
       "you              0       0    0    0     0  0        0     0   0    0  ...   \n",
       "\n",
       "          talk  tell  that  the  voice  was  which  woman,  yet  you  \n",
       "American     0     0     0    0      0    0      0       0    0    0  \n",
       "Giving       0     0     0    0      0    0      0       0    0    0  \n",
       "How          0     0     0    0      0    0      0       0    0    0  \n",
       "How          0     0     0    0      0    0      0       0    0    0  \n",
       "The          0     0     0    0      0    0      0       0    0    0  \n",
       "...        ...   ...   ...  ...    ...  ...    ...     ...  ...  ...  \n",
       "was          0     0     0    0      0    1      0       0    0    0  \n",
       "which        0     0     0    0      0    0      1       0    0    0  \n",
       "woman,       0     0     0    0      0    0      0       1    0    0  \n",
       "yet          0     0     0    0      0    0      0       0    1    0  \n",
       "you          0     0     0    0      0    0      0       0    0    1  \n",
       "\n",
       "[62 rows x 47 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab, index=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {},
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Lets now carry on building the bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "anonymous-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American', 1),\n",
       " ('Giving', 1),\n",
       " ('How', 1),\n",
       " ('The', 1),\n",
       " ('This', 1),\n",
       " ('a', 1),\n",
       " ('accent.', 1),\n",
       " ('also', 1),\n",
       " ('an', 1),\n",
       " ('and', 1),\n",
       " ('be', 1),\n",
       " ('can', 1),\n",
       " (\"can't\", 1),\n",
       " ('could', 1),\n",
       " ('definitely', 1),\n",
       " ('genders?', 1),\n",
       " ('good', 1),\n",
       " ('had', 1),\n",
       " ('have', 1),\n",
       " ('humanoid', 1),\n",
       " ('hurt', 1),\n",
       " ('idea,', 1),\n",
       " ('identities?', 1),\n",
       " ('it?', 1),\n",
       " ('like', 1),\n",
       " ('little.', 1),\n",
       " ('machine', 1),\n",
       " ('machine,', 1),\n",
       " ('machines', 1),\n",
       " ('making', 1),\n",
       " ('mind', 1),\n",
       " ('my', 1),\n",
       " ('nationalities?', 1),\n",
       " ('navigated', 1),\n",
       " ('of', 1),\n",
       " ('people,', 1),\n",
       " ('real', 1),\n",
       " ('talk', 1),\n",
       " ('tell', 1),\n",
       " ('that', 1),\n",
       " ('the', 1),\n",
       " ('voice', 1),\n",
       " ('was', 1),\n",
       " ('which', 1),\n",
       " ('woman,', 1),\n",
       " ('yet', 1),\n",
       " ('you', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {} # setting this up as a dictionary\n",
    "\n",
    "for token in tokens:\n",
    "    bow[token] = 1\n",
    "\n",
    "sorted(bow.items()) # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that same words will not duplicate.\n",
    "Pandas also has a more efficient form of a dictionary called Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adequate-aquatic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>American</th>\n",
       "      <th>Giving</th>\n",
       "      <th>How</th>\n",
       "      <th>The</th>\n",
       "      <th>This</th>\n",
       "      <th>a</th>\n",
       "      <th>accent.</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>...</th>\n",
       "      <th>talk</th>\n",
       "      <th>tell</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>voice</th>\n",
       "      <th>was</th>\n",
       "      <th>which</th>\n",
       "      <th>woman,</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      American  Giving  How  The  This  a  accent.  also  an  and  ...  talk  \\\n",
       "sent         1       1    1    1     1  1        1     1   1    1  ...     1   \n",
       "\n",
       "      tell  that  the  voice  was  which  woman,  yet  you  \n",
       "sent     1     1    1      1    1      1       1    1    1  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "senior-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>voice</th>\n",
       "      <th>that</th>\n",
       "      <th>navigated</th>\n",
       "      <th>was</th>\n",
       "      <th>definitely</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>machine,</th>\n",
       "      <th>and</th>\n",
       "      <th>...</th>\n",
       "      <th>idea,</th>\n",
       "      <th>making</th>\n",
       "      <th>talk</th>\n",
       "      <th>like</th>\n",
       "      <th>real</th>\n",
       "      <th>people,</th>\n",
       "      <th>it?</th>\n",
       "      <th>Giving</th>\n",
       "      <th>humanoid</th>\n",
       "      <th>identities?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       The  voice  that  navigated  was  definitely  of  a  machine,  and  \\\n",
       "sent0    1      1     1          1    1           1   1  1         1    1   \n",
       "sent1    0      0     0          0    0           0   0  0         0    0   \n",
       "sent2    1      0     0          0    0           0   0  0         0    0   \n",
       "sent3    0      0     0          0    0           0   0  0         0    0   \n",
       "sent4    0      0     0          0    0           0   0  1         0    0   \n",
       "sent5    0      0     0          0    0           0   0  0         0    0   \n",
       "\n",
       "       ...  idea,  making  talk  like  real  people,  it?  Giving  humanoid  \\\n",
       "sent0  ...      0       0     0     0     0        0    0       0         0   \n",
       "sent1  ...      0       0     0     0     0        0    0       0         0   \n",
       "sent2  ...      0       0     0     0     0        0    0       0         0   \n",
       "sent3  ...      0       0     0     0     0        0    0       0         0   \n",
       "sent4  ...      1       1     1     1     1        1    1       0         0   \n",
       "sent5  ...      0       0     0     0     0        0    0       1         1   \n",
       "\n",
       "       identities?  \n",
       "sent0            0  \n",
       "sent1            0  \n",
       "sent2            0  \n",
       "sent3            0  \n",
       "sent4            0  \n",
       "sent5            1  \n",
       "\n",
       "[6 rows x 47 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {},
   "source": [
    "Now we see how we managed to build feature vectors for the sentences we originally had. Now lets do a Dot Product calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "computational-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of sent0 from sent1: 0  and dot product of sent0 from sent1: 2\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(\"dot product of sent0 from sent1:\", df.sent0.dot(df.sent1), \" and dot product of sent0 from sent1:\", df.sent0.dot(df.sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171dc7f-c145-40f1-9138-1543079b1d6c",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Lets do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "saving-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'voice',\n",
       " 'that',\n",
       " 'navigated',\n",
       " 'was',\n",
       " 'definitely',\n",
       " 'that',\n",
       " 'of',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'could',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'was',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'which',\n",
       " 'hurt',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'a',\n",
       " 'little',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'genders',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'also',\n",
       " 'had',\n",
       " 'an',\n",
       " 'American',\n",
       " 'accent',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'nationalities',\n",
       " 'This',\n",
       " \"can't\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'making',\n",
       " 'machines',\n",
       " 'talk',\n",
       " 'like',\n",
       " 'real',\n",
       " 'people',\n",
       " 'can',\n",
       " 'it',\n",
       " 'Giving',\n",
       " 'machines',\n",
       " 'humanoid',\n",
       " 'identities',\n",
       " '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-finger",
   "metadata": {},
   "source": [
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Lets try NLTK lib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-timber",
   "metadata": {},
   "source": [
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prime-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'voice',\n",
       " 'that',\n",
       " 'navigated',\n",
       " 'was',\n",
       " 'definitely',\n",
       " 'that',\n",
       " 'of',\n",
       " 'a',\n",
       " 'machine',\n",
       " ',',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'could',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'was',\n",
       " 'a',\n",
       " 'woman',\n",
       " ',',\n",
       " 'which',\n",
       " 'hurt',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'a',\n",
       " 'little',\n",
       " '.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'genders',\n",
       " '?',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'also',\n",
       " 'had',\n",
       " 'an',\n",
       " 'American',\n",
       " 'accent',\n",
       " '.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'nationalities',\n",
       " '?',\n",
       " 'This',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea',\n",
       " ',',\n",
       " 'making',\n",
       " 'machines',\n",
       " 'talk',\n",
       " 'like',\n",
       " 'real',\n",
       " 'people',\n",
       " ',',\n",
       " 'can',\n",
       " 'it',\n",
       " '?',\n",
       " 'Giving',\n",
       " 'machines',\n",
       " 'humanoid',\n",
       " 'identities',\n",
       " '?']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {},
   "source": [
    "but there are other more specialised tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rapid-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'voice',\n",
       " 'that',\n",
       " 'navigated',\n",
       " 'was',\n",
       " 'definitely',\n",
       " 'that',\n",
       " 'of',\n",
       " 'a',\n",
       " 'machine',\n",
       " ',',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'could',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'was',\n",
       " 'a',\n",
       " 'woman',\n",
       " ',',\n",
       " 'which',\n",
       " 'hurt',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'a',\n",
       " 'little.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'genders',\n",
       " '?',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'also',\n",
       " 'had',\n",
       " 'an',\n",
       " 'American',\n",
       " 'accent.',\n",
       " 'How',\n",
       " 'can',\n",
       " 'machines',\n",
       " 'have',\n",
       " 'nationalities',\n",
       " '?',\n",
       " 'This',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea',\n",
       " ',',\n",
       " 'making',\n",
       " 'machines',\n",
       " 'talk',\n",
       " 'like',\n",
       " 'real',\n",
       " 'people',\n",
       " ',',\n",
       " 'can',\n",
       " 'it',\n",
       " '?',\n",
       " 'Giving',\n",
       " 'machines',\n",
       " 'humanoid',\n",
       " 'identities',\n",
       " '?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {},
   "source": [
    "For now lets use the regular expression special word pattern w, so we can controll what we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "south-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'voice', 'that', 'navigated', 'was', 'definitely', 'that', 'of', 'a', 'machine', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machine', 'was', 'a', 'woman', 'which', 'hurt', 'my', 'mind', 'a', 'little', 'How', 'can', 'machines', 'have', 'genders', 'The', 'machine', 'also', 'had', 'an', 'American', 'accent', 'How', 'can', 'machines', 'have', 'nationalities', 'This', 'can', 't', 'be', 'a', 'good', 'idea', 'making', 'machines', 'talk', 'like', 'real', 'people', 'can', 'it', 'Giving', 'machines', 'humanoid', 'identities']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {},
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331b82b-5bcc-47d7-9e92-7050394beb07",
   "metadata": {},
   "source": [
    "### To-Do. Use nltk's TweetTokenizer and summarize your observations on how it is different from other tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b099f-8fcb-43d2-9e8e-1452981c7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TweetTokenizer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce706947-0d2c-4be8-af5e-552ae07a0cb6",
   "metadata": {},
   "source": [
    "# Summarize your observation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c783821-ab7f-4692-abb7-83da5a553004",
   "metadata": {},
   "source": [
    "### To-Do. Use nltk's PunktSentenceTokenizer and summarize your observations on how it is different from other tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036d9af-8d95-4d14-9180-d23a161b570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PunktSentenceTokenizer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1ed2f-1ae4-49a2-9bf9-efe1942de7be",
   "metadata": {},
   "source": [
    "# Summarize your observation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe1ecd-bbfb-4a4f-bf7d-d241c848e09c",
   "metadata": {},
   "source": [
    "### To-Do. Use nltk's MWETokenizer and summarize your observations on how it is different from other tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1697b-733a-4f58-bda6-b6b04e9360f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MWETokenizer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002051c7-22ec-4329-aea9-762dc23e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize your observation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39f46f-bcfa-4f16-96ef-31505fca84ac",
   "metadata": {},
   "source": [
    "### n-Gram Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-invalid",
   "metadata": {},
   "source": [
    "We will now calculate the 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "guilty-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'voice'),\n",
       " ('voice', 'that'),\n",
       " ('that', 'navigated'),\n",
       " ('navigated', 'was'),\n",
       " ('was', 'definitely'),\n",
       " ('definitely', 'that'),\n",
       " ('that', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'machine'),\n",
       " ('machine', 'and'),\n",
       " ('and', 'yet'),\n",
       " ('yet', 'you'),\n",
       " ('you', 'could'),\n",
       " ('could', 'tell'),\n",
       " ('tell', 'that'),\n",
       " ('that', 'the'),\n",
       " ('the', 'machine'),\n",
       " ('machine', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'woman'),\n",
       " ('woman', 'which'),\n",
       " ('which', 'hurt'),\n",
       " ('hurt', 'my'),\n",
       " ('my', 'mind'),\n",
       " ('mind', 'a'),\n",
       " ('a', 'little'),\n",
       " ('little', 'How'),\n",
       " ('How', 'can'),\n",
       " ('can', 'machines'),\n",
       " ('machines', 'have'),\n",
       " ('have', 'genders'),\n",
       " ('genders', 'The'),\n",
       " ('The', 'machine'),\n",
       " ('machine', 'also'),\n",
       " ('also', 'had'),\n",
       " ('had', 'an'),\n",
       " ('an', 'American'),\n",
       " ('American', 'accent'),\n",
       " ('accent', 'How'),\n",
       " ('How', 'can'),\n",
       " ('can', 'machines'),\n",
       " ('machines', 'have'),\n",
       " ('have', 'nationalities'),\n",
       " ('nationalities', 'This'),\n",
       " ('This', 'can'),\n",
       " ('can', 't'),\n",
       " ('t', 'be'),\n",
       " ('be', 'a'),\n",
       " ('a', 'good'),\n",
       " ('good', 'idea'),\n",
       " ('idea', 'making'),\n",
       " ('making', 'machines'),\n",
       " ('machines', 'talk'),\n",
       " ('talk', 'like'),\n",
       " ('like', 'real'),\n",
       " ('real', 'people'),\n",
       " ('people', 'can'),\n",
       " ('can', 'it'),\n",
       " ('it', 'Giving'),\n",
       " ('Giving', 'machines'),\n",
       " ('machines', 'humanoid'),\n",
       " ('humanoid', 'identities')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {},
   "source": [
    "and 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "imperial-retreat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'voice', 'that'),\n",
       " ('voice', 'that', 'navigated'),\n",
       " ('that', 'navigated', 'was'),\n",
       " ('navigated', 'was', 'definitely'),\n",
       " ('was', 'definitely', 'that'),\n",
       " ('definitely', 'that', 'of'),\n",
       " ('that', 'of', 'a'),\n",
       " ('of', 'a', 'machine'),\n",
       " ('a', 'machine', 'and'),\n",
       " ('machine', 'and', 'yet'),\n",
       " ('and', 'yet', 'you'),\n",
       " ('yet', 'you', 'could'),\n",
       " ('you', 'could', 'tell'),\n",
       " ('could', 'tell', 'that'),\n",
       " ('tell', 'that', 'the'),\n",
       " ('that', 'the', 'machine'),\n",
       " ('the', 'machine', 'was'),\n",
       " ('machine', 'was', 'a'),\n",
       " ('was', 'a', 'woman'),\n",
       " ('a', 'woman', 'which'),\n",
       " ('woman', 'which', 'hurt'),\n",
       " ('which', 'hurt', 'my'),\n",
       " ('hurt', 'my', 'mind'),\n",
       " ('my', 'mind', 'a'),\n",
       " ('mind', 'a', 'little'),\n",
       " ('a', 'little', 'How'),\n",
       " ('little', 'How', 'can'),\n",
       " ('How', 'can', 'machines'),\n",
       " ('can', 'machines', 'have'),\n",
       " ('machines', 'have', 'genders'),\n",
       " ('have', 'genders', 'The'),\n",
       " ('genders', 'The', 'machine'),\n",
       " ('The', 'machine', 'also'),\n",
       " ('machine', 'also', 'had'),\n",
       " ('also', 'had', 'an'),\n",
       " ('had', 'an', 'American'),\n",
       " ('an', 'American', 'accent'),\n",
       " ('American', 'accent', 'How'),\n",
       " ('accent', 'How', 'can'),\n",
       " ('How', 'can', 'machines'),\n",
       " ('can', 'machines', 'have'),\n",
       " ('machines', 'have', 'nationalities'),\n",
       " ('have', 'nationalities', 'This'),\n",
       " ('nationalities', 'This', 'can'),\n",
       " ('This', 'can', 't'),\n",
       " ('can', 't', 'be'),\n",
       " ('t', 'be', 'a'),\n",
       " ('be', 'a', 'good'),\n",
       " ('a', 'good', 'idea'),\n",
       " ('good', 'idea', 'making'),\n",
       " ('idea', 'making', 'machines'),\n",
       " ('making', 'machines', 'talk'),\n",
       " ('machines', 'talk', 'like'),\n",
       " ('talk', 'like', 'real'),\n",
       " ('like', 'real', 'people'),\n",
       " ('real', 'people', 'can'),\n",
       " ('people', 'can', 'it'),\n",
       " ('can', 'it', 'Giving'),\n",
       " ('it', 'Giving', 'machines'),\n",
       " ('Giving', 'machines', 'humanoid'),\n",
       " ('machines', 'humanoid', 'identities')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {},
   "source": [
    "If we want to include the n-grams as a string rather than touples, then we need to convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "twenty-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The voice', 'voice that', 'that navigated', 'navigated was', 'was definitely', 'definitely that', 'that of', 'of a', 'a machine', 'machine and', 'and yet', 'yet you', 'you could', 'could tell', 'tell that', 'that the', 'the machine', 'machine was', 'was a', 'a woman', 'woman which', 'which hurt', 'hurt my', 'my mind', 'mind a', 'a little', 'little How', 'How can', 'can machines', 'machines have', 'have genders', 'genders The', 'The machine', 'machine also', 'also had', 'had an', 'an American', 'American accent', 'accent How', 'How can', 'can machines', 'machines have', 'have nationalities', 'nationalities This', 'This can', 'can t', 't be', 'be a', 'a good', 'good idea', 'idea making', 'making machines', 'machines talk', 'talk like', 'like real', 'real people', 'people can', 'can it', 'it Giving', 'Giving machines', 'machines humanoid', 'humanoid identities']\n",
      "\n",
      "['The voice that', 'voice that navigated', 'that navigated was', 'navigated was definitely', 'was definitely that', 'definitely that of', 'that of a', 'of a machine', 'a machine and', 'machine and yet', 'and yet you', 'yet you could', 'you could tell', 'could tell that', 'tell that the', 'that the machine', 'the machine was', 'machine was a', 'was a woman', 'a woman which', 'woman which hurt', 'which hurt my', 'hurt my mind', 'my mind a', 'mind a little', 'a little How', 'little How can', 'How can machines', 'can machines have', 'machines have genders', 'have genders The', 'genders The machine', 'The machine also', 'machine also had', 'also had an', 'had an American', 'an American accent', 'American accent How', 'accent How can', 'How can machines', 'can machines have', 'machines have nationalities', 'have nationalities This', 'nationalities This can', 'This can t', 'can t be', 't be a', 'be a good', 'a good idea', 'good idea making', 'idea making machines', 'making machines talk', 'machines talk like', 'talk like real', 'like real people', 'real people can', 'people can it', 'can it Giving', 'it Giving machines', 'Giving machines humanoid', 'machines humanoid identities']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(bigrams)\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print()\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {},
   "source": [
    "Another important step we looked at in the lectures are the stop words. Lets try to use the nltk stopword list to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0dfd2-2f8d-40c6-9631-655dba69d759",
   "metadata": {},
   "source": [
    "### Stop-word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-boston",
   "metadata": {},
   "source": [
    "First lets download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "apparent-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/diptesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {},
   "source": [
    "and now check it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pending-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"number of stopwords:\", len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Lets see a much larger set from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "monthly-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "frozenset({'interest', 'twenty', 'beyond', 'some', 'during', 'so', 'any', 'they', 'becomes', 'under', 'hence', 'nevertheless', 'onto', 'without', 'within', 'at', 'system', 'nothing', 'bottom', 'go', 'many', 'another', 'same', 'thereby', 'all', 'back', 'these', 'next', 'a', 'how', 'no', 'see', 'find', 'out', 'to', 'every', 'side', 'noone', 'sixty', 'been', 'become', 'last', 'thereafter', 'about', 'often', 'seemed', 'co', 'yourself', 'himself', 'though', 'thick', 'etc', 'his', 'myself', 'nowhere', 'being', 'what', 'everywhere', 'de', 'get', 'could', 'eg', 'toward', 'into', 'still', 'top', 'front', 'off', 'sometimes', 'anyway', 'on', 'latterly', 'she', 'please', 'although', 'from', 'herein', 'hereupon', 'i', 'afterwards', 'eight', 'whereafter', 'are', 'has', 'here', 'yet', 'indeed', 'can', 'empty', 'cant', 'amongst', 'but', 'describe', 'therefore', 'therein', 'where', 'almost', 'via', 'give', 'am', 'elsewhere', 'more', 'that', 'alone', 'if', 'former', 'even', 'than', 'when', 'namely', 'whereupon', 'however', 'done', 'above', 'cannot', 'one', 'name', 'wherever', 'fifty', 'before', 'amount', 'neither', 'seeming', 'hereafter', 'somehow', 'fifteen', 'among', 'you', 'also', 'moreover', 'four', 'first', 'those', 'had', 'nobody', 'take', 'hereby', 'ours', 'whence', 'yours', 'while', 'seem', 'becoming', 'my', 'together', 'twelve', 'should', 'through', 'whereby', 'enough', 'other', 'yourselves', 'and', 'ie', 'became', 'anywhere', 'up', 'cry', 'everyone', 'rather', 'throughout', 'me', 'thereupon', 'of', 'ourselves', 'because', 'anyhow', 'herself', 'be', 'sincere', 'mostly', 'in', 'it', 'whither', 'wherein', 'few', 'serious', 'such', 'others', 'only', 'would', 'once', 'eleven', 'amoungst', 'itself', 'were', 'we', 'least', 'call', 'six', 'upon', 'he', 'nor', 'this', 'three', 'someone', 'everything', 'which', 'again', 'detail', 'behind', 'less', 'whereas', 'otherwise', 'hasnt', 'very', 'an', 'thus', 'against', 'either', 'move', 'too', 'now', 'always', 'latter', 'there', 'mill', 'hundred', 'part', 'somewhere', 'several', 'or', 'ten', 'already', 'may', 'towards', 'own', 'meanwhile', 'between', 'two', 'have', 'never', 'its', 'whenever', 'might', 'nine', 'will', 'bill', 're', 'whoever', 'five', 'ever', 'further', 'put', 'since', 'due', 'much', 'who', 'most', 'around', 'must', 'made', 'until', 'after', 'not', 'inc', 'below', 'third', 'seems', 'besides', 'fill', 'the', 'keep', 'whom', 'ltd', 'each', 'do', 'across', 'except', 'was', 'both', 'down', 'show', 'thence', 'perhaps', 'mine', 'whole', 'us', 'thru', 'formerly', 'why', 'couldnt', 'along', 'hers', 'as', 'sometime', 'anyone', 'him', 'their', 'well', 'for', 'none', 'our', 'over', 'your', 'whether', 'with', 'else', 'beforehand', 'beside', 'something', 'is', 'forty', 'themselves', 'fire', 'con', 'whatever', 'them', 'found', 'by', 'thin', 'un', 'anything', 'her', 'whose', 'then', 'full', 'per'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(\"number of stopwords:\", len(sklearn_stop_words))\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {},
   "source": [
    "Strangely enough, although there are more stopwords in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the teo lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-hostel",
   "metadata": {},
   "source": [
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alive-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'voice', 'that', 'navigated', 'was', 'definitely', 'that', 'of', 'a', 'machine', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machine', 'was', 'a', 'woman', 'which', 'hurt', 'my', 'mind', 'a', 'little', 'how', 'can', 'machines', 'have', 'genders', 'the', 'machine', 'also', 'had', 'an', 'american', 'accent', 'how', 'can', 'machines', 'have', 'nationalities', 'this', 'can', 't', 'be', 'a', 'good', 'idea', 'making', 'machines', 'talk', 'like', 'real', 'people', 'can', 'it', 'giving', 'machines', 'humanoid', 'identities']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262d027-b7e0-499c-816a-0e31e6755c77",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {},
   "source": [
    "For stemming the words we could use nltk again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "upset-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'voic', 'that', 'navig', 'wa', 'definit', 'that', 'of', 'a', 'machin', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machin', 'wa', 'a', 'woman', 'which', 'hurt', 'my', 'mind', 'a', 'littl', 'how', 'can', 'machin', 'have', 'gender', 'the', 'machin', 'also', 'had', 'an', 'american', 'accent', 'how', 'can', 'machin', 'have', 'nation', 'thi', 'can', 't', 'be', 'a', 'good', 'idea', 'make', 'machin', 'talk', 'like', 'real', 'peopl', 'can', 'it', 'give', 'machin', 'humanoid', 'ident']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {},
   "source": [
    "For lemmatising nltk again also do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "desperate-diploma",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/diptesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'voice', 'that', 'navigated', 'wa', 'definitely', 'that', 'of', 'a', 'machine', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machine', 'wa', 'a', 'woman', 'which', 'hurt', 'my', 'mind', 'a', 'little', 'how', 'can', 'machine', 'have', 'gender', 'the', 'machine', 'also', 'had', 'an', 'american', 'accent', 'how', 'can', 'machine', 'have', 'nationality', 'this', 'can', 't', 'be', 'a', 'good', 'idea', 'making', 'machine', 'talk', 'like', 'real', 'people', 'can', 'it', 'giving', 'machine', 'humanoid', 'identity']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {},
   "source": [
    "The sentence we have has no issues with the lemma... but look into the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "understanding-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", 'a')) # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {},
   "source": [
    "If we don't include the POS, the nltk library with wordnet does not work well. So lets try fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "russian-merit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/diptesh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "imperial-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'voice', 'that', 'navigate', 'be', 'definitely', 'that', 'of', 'a', 'machine', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machine', 'be', 'a', 'woman', 'which', 'hurt', 'my', 'mind', 'a', 'little', 'how', 'can', 'machine', 'have', 'gender', 'the', 'machine', 'also', 'have', 'an', 'american', 'accent', 'how', 'can', 'machine', 'have', 'nationality', 'this', 'can', 't', 'be', 'a', 'good', 'idea', 'make', 'machine', 'talk', 'like', 'real', 'people', 'can', 'it', 'give', 'machine', 'humanoid', 'identity']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9e7ec-0763-4a9c-a60a-ebda41988eaa",
   "metadata": {},
   "source": [
    "### Feature-vector Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "polished-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 3,\n",
       "         'voice': 1,\n",
       "         'that': 3,\n",
       "         'navigate': 1,\n",
       "         'be': 3,\n",
       "         'definitely': 1,\n",
       "         'of': 1,\n",
       "         'a': 4,\n",
       "         'machine': 7,\n",
       "         'and': 1,\n",
       "         'yet': 1,\n",
       "         'you': 1,\n",
       "         'could': 1,\n",
       "         'tell': 1,\n",
       "         'woman': 1,\n",
       "         'which': 1,\n",
       "         'hurt': 1,\n",
       "         'my': 1,\n",
       "         'mind': 1,\n",
       "         'little': 1,\n",
       "         'how': 2,\n",
       "         'can': 4,\n",
       "         'have': 3,\n",
       "         'gender': 1,\n",
       "         'also': 1,\n",
       "         'an': 1,\n",
       "         'american': 1,\n",
       "         'accent': 1,\n",
       "         'nationality': 1,\n",
       "         'this': 1,\n",
       "         't': 1,\n",
       "         'good': 1,\n",
       "         'idea': 1,\n",
       "         'make': 1,\n",
       "         'talk': 1,\n",
       "         'like': 1,\n",
       "         'real': 1,\n",
       "         'people': 1,\n",
       "         'it': 1,\n",
       "         'give': 1,\n",
       "         'humanoid': 1,\n",
       "         'identity': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {},
   "source": [
    "Now lets check the most frequent 6 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "unable-champagne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine', 7), ('a', 4), ('can', 4), ('the', 3), ('that', 3), ('be', 3)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {},
   "source": [
    "Now lets remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "graduate-showcase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'voice': 1,\n",
       "         'navigate': 1,\n",
       "         'definitely': 1,\n",
       "         'machine': 7,\n",
       "         'yet': 1,\n",
       "         'could': 1,\n",
       "         'tell': 1,\n",
       "         'woman': 1,\n",
       "         'hurt': 1,\n",
       "         'mind': 1,\n",
       "         'little': 1,\n",
       "         'gender': 1,\n",
       "         'also': 1,\n",
       "         'american': 1,\n",
       "         'accent': 1,\n",
       "         'nationality': 1,\n",
       "         'good': 1,\n",
       "         'idea': 1,\n",
       "         'make': 1,\n",
       "         'talk': 1,\n",
       "         'like': 1,\n",
       "         'real': 1,\n",
       "         'people': 1,\n",
       "         'give': 1,\n",
       "         'humanoid': 1,\n",
       "         'identity': 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {},
   "source": [
    "Finally... lets make our feature vector using the frequency ratio (term count / total number of terms in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "retained-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21875, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {},
   "source": [
    "We have explored many many options already and we will continue with more advances feature vectors in the next lab, plus some visualisations in charts. So untill then please try different experiments on your own:\n",
    "* see if you change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "* try to use different libraries for tokenising , PoS, stemming and lemmatising\n",
    "* try to use other distance metrics to compare vectors, such as Euclidian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d683135-5f2b-4324-8acc-6939a0055d56",
   "metadata": {},
   "source": [
    "### To-Do : Create document vectors without stop-word removal, with different tokenizing and stemming algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20868a-6a86-4c2f-8b2d-d76fcaad1950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
