{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {},
   "source": [
    "## Introduction to Lab 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {},
   "source": [
    "This lab is an introduction to some basic test processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-shuttle",
   "metadata": {},
   "source": [
    "So lets first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The voice that navigated was definitely that of a machine, and yet you could tell that the machine was a woman, which hurt my mind a little.\\n How can machines have genders?\\n The machine also had an American accent.\\n How can machines have nationalities?\\n This can't be a good idea, making machines talk like real people, can it?\\n Giving machines humanoid identities?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {},
   "source": [
    "Now lets try to split the text based on spaces (default function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {},
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended to the words.\n",
    "So we need to find a way to remove those characters... but, before we do that, lets see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(sentence.split()) # splitting based on spaces\n",
    "vocab = sorted(set(tokens)) # sorting and removing duplicates by using set()\n",
    "vocab # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {},
   "source": [
    "We can see that the order has the numbers first, followerd by capital and then lower case letters (all alphabetically sorted). We also see that some repeating words appear only once in the vocabulary list. Let's compate the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(\"tokens:\", tokens_len)\n",
    "print(\"vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {},
   "source": [
    "Lets try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {},
   "source": [
    "Is not easy to see, but some columns contain multiple rows showing 1, whereas the rest is all one 1 per column. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab, index=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {},
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Lets now carry on building the bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = {} # setting this up as a dictionary\n",
    "\n",
    "for token in tokens:\n",
    "    bow[token] = 1\n",
    "\n",
    "sorted(bow.items()) # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that same words will not duplicate.\n",
    "Pandas also has a more efficient form of a dictionary called Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {},
   "source": [
    "Now we see how we managed to build feature vectors for the sentences we originally had. Now lets do a Dot Product calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T\n",
    "print(\"dot product of sent0 from sent1:\", df.sent0.dot(df.sent1), \" and dot product of sent0 from sent1:\", df.sent0.dot(df.sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171dc7f-c145-40f1-9138-1543079b1d6c",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Lets do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-finger",
   "metadata": {},
   "source": [
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Lets try NLTK lib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-timber",
   "metadata": {},
   "source": [
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {},
   "source": [
    "but there are other more specialised tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {},
   "source": [
    "For now lets use the regular expression special word pattern w, so we can controll what we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {},
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39f46f-bcfa-4f16-96ef-31505fca84ac",
   "metadata": {},
   "source": [
    "### n-Gram Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-invalid",
   "metadata": {},
   "source": [
    "We will now calculate the 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {},
   "source": [
    "and 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {},
   "source": [
    "If we want to include the n-grams as a string rather than touples, then we need to convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(bigrams)\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print()\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {},
   "source": [
    "Another important step we looked at in the lectures are the stop words. Lets try to use the nltk stopword list to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0dfd2-2f8d-40c6-9631-655dba69d759",
   "metadata": {},
   "source": [
    "### Stop-word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-boston",
   "metadata": {},
   "source": [
    "First lets download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {},
   "source": [
    "and now check it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"number of stopwords:\", len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Lets see a much larger set from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(\"number of stopwords:\", len(sklearn_stop_words))\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {},
   "source": [
    "Strangely enough, although there are more stopwords in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the teo lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-hostel",
   "metadata": {},
   "source": [
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262d027-b7e0-499c-816a-0e31e6755c77",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {},
   "source": [
    "For stemming the words we could use nltk again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {},
   "source": [
    "For lemmatising nltk again also do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {},
   "source": [
    "The sentence we have has no issues with the lemma... but look into the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", 'a')) # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {},
   "source": [
    "If we don't include the POS, the nltk library with wordnet does not work well. So lets try fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9e7ec-0763-4a9c-a60a-ebda41988eaa",
   "metadata": {},
   "source": [
    "### Feature-vector Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {},
   "source": [
    "Now lets check the most frequent 6 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {},
   "source": [
    "Now lets remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {},
   "source": [
    "Finally... lets make our feature vector using the frequency ratio (term count / total number of terms in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {},
   "source": [
    "We have explored many many options already and we will continue with more advances feature vectors in the next lab, plus some visualisations in charts. So untill then please try different experiments on your own:\n",
    "* see if you change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "* try to use different libraries for tokenising , PoS, stemming and lemmatising\n",
    "* try to use other distance metrics to compare vectors, such as Euclidian distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
